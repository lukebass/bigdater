1) The only fields loaded were the ones the program used: score and subreddit. It appeared as though the average was computed in two steps: 
first it performed a "partial_avg" and then the complete "avg". There was an intermediate step between the two, in which it was exchanging 
the hashpartitioning. This seems similar to a combiner-like step: taking the partial averages and combining them to find the complete average.

2) For the following tests I used the reddit-5 dataset:

MapReduce: 2m6.604s
RDDs and Python3: 3m19.393s
DataFrames and Python3: 1m39.970s
RDDs and PyPy: 2m0.070s
DataFrames and PyPy: 0m55.417s

Using PyPy sped up the program execution for both RDDs and DataFrames, however moreso for DataFrames. I assume the time reduction was larger 
for the RDD implementation because that implementation required performing more Python operations on the data. In the DataFrames implementation, 
we read in the data, get the averages, then write the data out. There are less operations here for PyPy's just-in-time compiler to optimize.


3) For the following tests I used the pagecounts-3 dataset:

Without broadcast: 1m46.676s -- 2m17.074s
With broadcast: 1m35.989s

4) The execution plan decreased from 7 to 4 steps when using the broadcast hint. Furthermore, when we examine the execution plans further, we 
can see that the type of join changed from a SortMergeJoin to a BroadcastHashJoin, which is presumably faster.

5) Honestly, it took me less time to write temp_range_sql.py, as I am more familiar, at this point, with SQL. There was definitely a learning 
curve regarding getting a handle on DataFrames. That being said, the DataFrames methodology seems more functionally driven. It reminds me of 
writing Haskell, or other functional languages. Once accustomed, I believe that using DataFrames produces significantly more readable code.