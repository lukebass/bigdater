1) The wordcount-5 data set was worth repartitioning because the 8 input files were of significantly different sizes. This means that the work 
each executor has to do varies based on the size of the partition. By repartitioning, we ensure that each partition, of our RDD of strings, are 
all the same size. Therefore, the workload is balanced across executors, which significantly decreases the runtime.

2) The same fix does not work for the wordcount-3 data set because there is a cost associated with repartitioning data. First, if we examine 
the input files, we note that they don't vary too drastically in size. Second, there are quite a few files and repartitioning from N to M 
partitions could be quite expensive. Therefore, since the size of each partition will be similar, repartitioning won't benefit us.

3) We could modify the wordcount-5 input by balancing the file sizes before running our program. Again, there is a cost associated with 
repartitioning data. If we were preparing a data set to be used in a project, for example, we would, ideally, like to optimize the initial data 
set, so that our program can focus on the task at hand. Equally sized files set us up for equal executor workloads.

4) When experimenting with the number of partitions, on my laptop, I found that anywhere between 2 to 40 partitions gave similar runtimes.

5) For each of the following tests I used n=100000000 samples:

Spark, Python3, and euler.py: 46.804s
Spark, PyPy, and euler.py: 11.107s
Spark, PyPy, and euler.py (--master=local[1]): 17.746s
PyPy and euler_single.py: 5.534s
C and euler.c: 2.982s

It seems as though Spark adds ~10s of overhead to a job. Running euler.py with Python3 took ~47s, while running euler.py with PyPy took ~11s. 
PyPy sped up the program execution by a factor of 4!